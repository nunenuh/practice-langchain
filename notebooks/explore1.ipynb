{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you laugh!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "llm.invoke(\"Tell me a joke!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Here's one:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\\n\\nHope that made you giggle! Do you want to hear another one?\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.batch([\"Tell me a joke\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "'s\n",
      " one\n",
      ":\n",
      "\n",
      "\n",
      "Why\n",
      " couldn\n",
      "'t\n",
      " the\n",
      " bicycle\n",
      " stand\n",
      " up\n",
      " by\n",
      " itself\n",
      "?\n",
      "\n",
      "\n",
      "Because\n",
      " it\n",
      " was\n",
      " two\n",
      "-t\n",
      "ired\n",
      "!\n",
      "\n",
      "\n",
      "Hope\n",
      " that\n",
      " brought\n",
      " a\n",
      " smile\n",
      " to\n",
      " your\n",
      " face\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"Tell me a joke\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why don't eggs tell jokes?\n",
      "\n",
      "(wait for it...)\n",
      "\n",
      "Because they'd crack each other up!\n",
      "\n",
      "Hope that made you smile! Do you want to hear another?\n",
      "[\"Here's one:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\\n\\nHope that made you smile!\"]\n",
      "Here\n",
      "'s\n",
      " one\n",
      ":\n",
      "\n",
      "\n",
      "Why\n",
      " don\n",
      "'t\n",
      " scientists\n",
      " trust\n",
      " atoms\n",
      "?\n",
      "\n",
      "\n",
      "Because\n",
      " they\n",
      " make\n",
      " up\n",
      " everything\n",
      "!\n",
      "\n",
      "\n",
      "Hope\n",
      " that\n",
      " made\n",
      " you\n",
      " gig\n",
      "gle\n",
      "!\n",
      " Do\n",
      " you\n",
      " want\n",
      " to\n",
      " hear\n",
      " another\n",
      " one\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(await llm.ainvoke(\"Tell me a joke\"))\n",
    "print(await llm.abatch([\"Tell me a joke\"]))\n",
    "async for chunk in llm.astream(\"Tell me a joke\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "Hope that made you smile!\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke([{\"content\": \"Tell me a joke\", \"type\": \"user\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you smile! Do you want to hear another?\" response_metadata={'model': 'llama3', 'created_at': '2024-05-17T00:19:50.194638249Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 12384900729, 'load_duration': 1158674, 'prompt_eval_count': 10, 'prompt_eval_duration': 1353907000, 'eval_count': 40, 'eval_duration': 10900374000} id='run-6dc8fc2d-74bc-4788-b80e-818154a988e1-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "chat_llm = ChatOllama(model=\"llama3\")\n",
    "print(chat_llm.invoke(\"tell me a joke.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
